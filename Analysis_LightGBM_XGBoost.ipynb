{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23sQk0UVCVei"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuaRvj0PCVe_",
    "outputId": "c8484f3c-c78c-4c99-eb86-fe525bfb64c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR_1</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>HR_3</th>\n",
       "      <th>HR_4</th>\n",
       "      <th>HR_5</th>\n",
       "      <th>EDA_1</th>\n",
       "      <th>EDA_2</th>\n",
       "      <th>EDA_3</th>\n",
       "      <th>EDA_4</th>\n",
       "      <th>EDA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>AU45_r_91</th>\n",
       "      <th>AU45_r_92</th>\n",
       "      <th>AU45_r_93</th>\n",
       "      <th>AU45_r_94</th>\n",
       "      <th>AU45_r_95</th>\n",
       "      <th>AU45_r_96</th>\n",
       "      <th>AU45_r_97</th>\n",
       "      <th>AU45_r_98</th>\n",
       "      <th>AU45_r_99</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.98</td>\n",
       "      <td>65.98</td>\n",
       "      <td>65.95</td>\n",
       "      <td>65.92</td>\n",
       "      <td>65.87</td>\n",
       "      <td>1.171049</td>\n",
       "      <td>1.168488</td>\n",
       "      <td>1.160803</td>\n",
       "      <td>1.160803</td>\n",
       "      <td>1.158241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68.67</td>\n",
       "      <td>68.85</td>\n",
       "      <td>69.02</td>\n",
       "      <td>69.20</td>\n",
       "      <td>69.37</td>\n",
       "      <td>1.265834</td>\n",
       "      <td>1.260710</td>\n",
       "      <td>1.253025</td>\n",
       "      <td>1.251744</td>\n",
       "      <td>1.250463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.67</td>\n",
       "      <td>89.95</td>\n",
       "      <td>90.20</td>\n",
       "      <td>90.43</td>\n",
       "      <td>90.68</td>\n",
       "      <td>1.479838</td>\n",
       "      <td>1.468310</td>\n",
       "      <td>1.461906</td>\n",
       "      <td>1.455501</td>\n",
       "      <td>1.455501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.87</td>\n",
       "      <td>74.98</td>\n",
       "      <td>75.10</td>\n",
       "      <td>75.20</td>\n",
       "      <td>75.30</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>0.092223</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>0.088380</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.55</td>\n",
       "      <td>75.57</td>\n",
       "      <td>75.60</td>\n",
       "      <td>75.67</td>\n",
       "      <td>75.72</td>\n",
       "      <td>0.092223</td>\n",
       "      <td>0.096065</td>\n",
       "      <td>0.116559</td>\n",
       "      <td>0.092223</td>\n",
       "      <td>0.093503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>75.62</td>\n",
       "      <td>75.92</td>\n",
       "      <td>76.23</td>\n",
       "      <td>76.52</td>\n",
       "      <td>76.78</td>\n",
       "      <td>0.103750</td>\n",
       "      <td>0.103750</td>\n",
       "      <td>0.107593</td>\n",
       "      <td>0.105031</td>\n",
       "      <td>0.102469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53.40</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.62</td>\n",
       "      <td>53.77</td>\n",
       "      <td>53.95</td>\n",
       "      <td>1.537292</td>\n",
       "      <td>1.547538</td>\n",
       "      <td>1.548818</td>\n",
       "      <td>1.547538</td>\n",
       "      <td>1.546257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>54.87</td>\n",
       "      <td>54.83</td>\n",
       "      <td>54.80</td>\n",
       "      <td>54.77</td>\n",
       "      <td>54.72</td>\n",
       "      <td>1.510397</td>\n",
       "      <td>1.503993</td>\n",
       "      <td>1.500151</td>\n",
       "      <td>1.509116</td>\n",
       "      <td>1.502713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>52.03</td>\n",
       "      <td>52.15</td>\n",
       "      <td>52.25</td>\n",
       "      <td>52.33</td>\n",
       "      <td>52.40</td>\n",
       "      <td>1.550099</td>\n",
       "      <td>1.551380</td>\n",
       "      <td>1.542415</td>\n",
       "      <td>1.542415</td>\n",
       "      <td>1.551380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64.18</td>\n",
       "      <td>64.20</td>\n",
       "      <td>64.22</td>\n",
       "      <td>64.20</td>\n",
       "      <td>64.23</td>\n",
       "      <td>0.089661</td>\n",
       "      <td>0.092223</td>\n",
       "      <td>0.092223</td>\n",
       "      <td>0.089661</td>\n",
       "      <td>0.088380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>65.17</td>\n",
       "      <td>65.40</td>\n",
       "      <td>65.58</td>\n",
       "      <td>65.82</td>\n",
       "      <td>66.05</td>\n",
       "      <td>0.085818</td>\n",
       "      <td>0.083256</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>0.083256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>69.82</td>\n",
       "      <td>69.87</td>\n",
       "      <td>69.88</td>\n",
       "      <td>69.88</td>\n",
       "      <td>69.87</td>\n",
       "      <td>0.076852</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.075571</td>\n",
       "      <td>0.074290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>90.27</td>\n",
       "      <td>90.42</td>\n",
       "      <td>90.57</td>\n",
       "      <td>90.72</td>\n",
       "      <td>90.83</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.085813</td>\n",
       "      <td>0.088375</td>\n",
       "      <td>0.087094</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>92.00</td>\n",
       "      <td>91.97</td>\n",
       "      <td>91.95</td>\n",
       "      <td>91.95</td>\n",
       "      <td>91.93</td>\n",
       "      <td>0.108867</td>\n",
       "      <td>0.108867</td>\n",
       "      <td>0.103744</td>\n",
       "      <td>0.108867</td>\n",
       "      <td>0.110148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>90.68</td>\n",
       "      <td>90.70</td>\n",
       "      <td>90.72</td>\n",
       "      <td>90.73</td>\n",
       "      <td>90.75</td>\n",
       "      <td>0.253597</td>\n",
       "      <td>0.257439</td>\n",
       "      <td>0.256158</td>\n",
       "      <td>0.256158</td>\n",
       "      <td>0.263843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>70.37</td>\n",
       "      <td>70.37</td>\n",
       "      <td>70.37</td>\n",
       "      <td>70.37</td>\n",
       "      <td>70.35</td>\n",
       "      <td>0.427974</td>\n",
       "      <td>0.430535</td>\n",
       "      <td>0.431815</td>\n",
       "      <td>0.427974</td>\n",
       "      <td>0.430535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>70.12</td>\n",
       "      <td>70.07</td>\n",
       "      <td>70.02</td>\n",
       "      <td>69.98</td>\n",
       "      <td>69.97</td>\n",
       "      <td>0.380593</td>\n",
       "      <td>0.378032</td>\n",
       "      <td>0.372910</td>\n",
       "      <td>0.370349</td>\n",
       "      <td>0.376751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>68.75</td>\n",
       "      <td>68.78</td>\n",
       "      <td>68.82</td>\n",
       "      <td>68.83</td>\n",
       "      <td>68.85</td>\n",
       "      <td>0.371629</td>\n",
       "      <td>0.372910</td>\n",
       "      <td>0.369068</td>\n",
       "      <td>0.369068</td>\n",
       "      <td>0.365226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>75.78</td>\n",
       "      <td>75.78</td>\n",
       "      <td>75.80</td>\n",
       "      <td>75.82</td>\n",
       "      <td>75.85</td>\n",
       "      <td>0.265124</td>\n",
       "      <td>0.267685</td>\n",
       "      <td>0.262562</td>\n",
       "      <td>0.266405</td>\n",
       "      <td>0.266405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>77.07</td>\n",
       "      <td>77.08</td>\n",
       "      <td>77.08</td>\n",
       "      <td>77.03</td>\n",
       "      <td>76.93</td>\n",
       "      <td>0.268966</td>\n",
       "      <td>0.268966</td>\n",
       "      <td>0.265124</td>\n",
       "      <td>0.267685</td>\n",
       "      <td>0.267685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>74.00</td>\n",
       "      <td>74.02</td>\n",
       "      <td>74.02</td>\n",
       "      <td>73.98</td>\n",
       "      <td>74.00</td>\n",
       "      <td>0.270247</td>\n",
       "      <td>0.270247</td>\n",
       "      <td>0.272808</td>\n",
       "      <td>0.272808</td>\n",
       "      <td>0.270247</td>\n",
       "      <td>...</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>64.23</td>\n",
       "      <td>64.08</td>\n",
       "      <td>63.92</td>\n",
       "      <td>63.73</td>\n",
       "      <td>63.60</td>\n",
       "      <td>0.309970</td>\n",
       "      <td>0.312532</td>\n",
       "      <td>0.313813</td>\n",
       "      <td>0.315094</td>\n",
       "      <td>0.311251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>63.58</td>\n",
       "      <td>63.55</td>\n",
       "      <td>63.48</td>\n",
       "      <td>63.37</td>\n",
       "      <td>63.20</td>\n",
       "      <td>0.283072</td>\n",
       "      <td>0.281791</td>\n",
       "      <td>0.283072</td>\n",
       "      <td>0.283072</td>\n",
       "      <td>0.280510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>67.52</td>\n",
       "      <td>67.90</td>\n",
       "      <td>68.25</td>\n",
       "      <td>68.72</td>\n",
       "      <td>69.20</td>\n",
       "      <td>0.299723</td>\n",
       "      <td>0.299723</td>\n",
       "      <td>0.301004</td>\n",
       "      <td>0.299723</td>\n",
       "      <td>0.301004</td>\n",
       "      <td>...</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.59</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>81.13</td>\n",
       "      <td>81.12</td>\n",
       "      <td>81.13</td>\n",
       "      <td>81.18</td>\n",
       "      <td>81.25</td>\n",
       "      <td>0.294849</td>\n",
       "      <td>0.297410</td>\n",
       "      <td>0.294849</td>\n",
       "      <td>0.293568</td>\n",
       "      <td>0.297410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>82.55</td>\n",
       "      <td>82.57</td>\n",
       "      <td>82.58</td>\n",
       "      <td>82.62</td>\n",
       "      <td>82.67</td>\n",
       "      <td>0.206474</td>\n",
       "      <td>0.206474</td>\n",
       "      <td>0.201351</td>\n",
       "      <td>0.197509</td>\n",
       "      <td>0.203913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>82.00</td>\n",
       "      <td>82.03</td>\n",
       "      <td>82.08</td>\n",
       "      <td>82.10</td>\n",
       "      <td>82.13</td>\n",
       "      <td>0.156523</td>\n",
       "      <td>0.157804</td>\n",
       "      <td>0.157804</td>\n",
       "      <td>0.159085</td>\n",
       "      <td>0.150119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>69.25</td>\n",
       "      <td>69.37</td>\n",
       "      <td>69.47</td>\n",
       "      <td>69.53</td>\n",
       "      <td>69.60</td>\n",
       "      <td>0.125518</td>\n",
       "      <td>0.122956</td>\n",
       "      <td>0.124237</td>\n",
       "      <td>0.126798</td>\n",
       "      <td>0.124237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>71.68</td>\n",
       "      <td>71.72</td>\n",
       "      <td>71.75</td>\n",
       "      <td>71.78</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.147291</td>\n",
       "      <td>0.124237</td>\n",
       "      <td>0.125518</td>\n",
       "      <td>0.125518</td>\n",
       "      <td>0.124237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>72.20</td>\n",
       "      <td>72.20</td>\n",
       "      <td>72.20</td>\n",
       "      <td>72.20</td>\n",
       "      <td>72.22</td>\n",
       "      <td>0.125518</td>\n",
       "      <td>0.130641</td>\n",
       "      <td>0.125518</td>\n",
       "      <td>0.125518</td>\n",
       "      <td>0.130641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>83.95</td>\n",
       "      <td>83.92</td>\n",
       "      <td>83.87</td>\n",
       "      <td>83.82</td>\n",
       "      <td>83.80</td>\n",
       "      <td>2.456486</td>\n",
       "      <td>2.461610</td>\n",
       "      <td>2.466733</td>\n",
       "      <td>2.464171</td>\n",
       "      <td>2.460329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>84.92</td>\n",
       "      <td>84.95</td>\n",
       "      <td>85.00</td>\n",
       "      <td>85.03</td>\n",
       "      <td>85.03</td>\n",
       "      <td>2.825376</td>\n",
       "      <td>2.799759</td>\n",
       "      <td>2.763895</td>\n",
       "      <td>2.729311</td>\n",
       "      <td>2.711379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>86.77</td>\n",
       "      <td>86.57</td>\n",
       "      <td>86.40</td>\n",
       "      <td>86.28</td>\n",
       "      <td>86.18</td>\n",
       "      <td>3.507408</td>\n",
       "      <td>3.502285</td>\n",
       "      <td>3.501004</td>\n",
       "      <td>3.504847</td>\n",
       "      <td>3.507408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows Ã— 2594 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HR_1   HR_2   HR_3   HR_4   HR_5     EDA_1     EDA_2     EDA_3     EDA_4  \\\n",
       "0   65.98  65.98  65.95  65.92  65.87  1.171049  1.168488  1.160803  1.160803   \n",
       "1   68.67  68.85  69.02  69.20  69.37  1.265834  1.260710  1.253025  1.251744   \n",
       "2   89.67  89.95  90.20  90.43  90.68  1.479838  1.468310  1.461906  1.455501   \n",
       "3   74.87  74.98  75.10  75.20  75.30  0.087099  0.092223  0.090942  0.088380   \n",
       "4   75.55  75.57  75.60  75.67  75.72  0.092223  0.096065  0.116559  0.092223   \n",
       "5   75.62  75.92  76.23  76.52  76.78  0.103750  0.103750  0.107593  0.105031   \n",
       "9   53.40  53.50  53.62  53.77  53.95  1.537292  1.547538  1.548818  1.547538   \n",
       "10  54.87  54.83  54.80  54.77  54.72  1.510397  1.503993  1.500151  1.509116   \n",
       "11  52.03  52.15  52.25  52.33  52.40  1.550099  1.551380  1.542415  1.542415   \n",
       "18  64.18  64.20  64.22  64.20  64.23  0.089661  0.092223  0.092223  0.089661   \n",
       "19  65.17  65.40  65.58  65.82  66.05  0.085818  0.083256  0.087099  0.087099   \n",
       "20  69.82  69.87  69.88  69.88  69.87  0.076852  0.073009  0.073009  0.075571   \n",
       "24  90.27  90.42  90.57  90.72  90.83  0.089655  0.085813  0.088375  0.087094   \n",
       "25  92.00  91.97  91.95  91.95  91.93  0.108867  0.108867  0.103744  0.108867   \n",
       "26  90.68  90.70  90.72  90.73  90.75  0.253597  0.257439  0.256158  0.256158   \n",
       "30  70.37  70.37  70.37  70.37  70.35  0.427974  0.430535  0.431815  0.427974   \n",
       "31  70.12  70.07  70.02  69.98  69.97  0.380593  0.378032  0.372910  0.370349   \n",
       "32  68.75  68.78  68.82  68.83  68.85  0.371629  0.372910  0.369068  0.369068   \n",
       "33  75.78  75.78  75.80  75.82  75.85  0.265124  0.267685  0.262562  0.266405   \n",
       "34  77.07  77.08  77.08  77.03  76.93  0.268966  0.268966  0.265124  0.267685   \n",
       "35  74.00  74.02  74.02  73.98  74.00  0.270247  0.270247  0.272808  0.272808   \n",
       "36  64.23  64.08  63.92  63.73  63.60  0.309970  0.312532  0.313813  0.315094   \n",
       "37  63.58  63.55  63.48  63.37  63.20  0.283072  0.281791  0.283072  0.283072   \n",
       "38  67.52  67.90  68.25  68.72  69.20  0.299723  0.299723  0.301004  0.299723   \n",
       "39  81.13  81.12  81.13  81.18  81.25  0.294849  0.297410  0.294849  0.293568   \n",
       "40  82.55  82.57  82.58  82.62  82.67  0.206474  0.206474  0.201351  0.197509   \n",
       "41  82.00  82.03  82.08  82.10  82.13  0.156523  0.157804  0.157804  0.159085   \n",
       "45  69.25  69.37  69.47  69.53  69.60  0.125518  0.122956  0.124237  0.126798   \n",
       "46  71.68  71.72  71.75  71.78  71.78  0.147291  0.124237  0.125518  0.125518   \n",
       "47  72.20  72.20  72.20  72.20  72.22  0.125518  0.130641  0.125518  0.125518   \n",
       "48  83.95  83.92  83.87  83.82  83.80  2.456486  2.461610  2.466733  2.464171   \n",
       "49  84.92  84.95  85.00  85.03  85.03  2.825376  2.799759  2.763895  2.729311   \n",
       "50  86.77  86.57  86.40  86.28  86.18  3.507408  3.502285  3.501004  3.504847   \n",
       "\n",
       "       EDA_5  ...     AU45_r_91   AU45_r_92   AU45_r_93   AU45_r_94  \\\n",
       "0   1.158241  ...          0.55        0.60        0.71        0.69   \n",
       "1   1.250463  ...          0.05        0.05        0.00        0.09   \n",
       "2   1.455501  ...          0.00        0.00        0.00        0.00   \n",
       "3   0.087099  ...          0.14        0.12        0.07        0.02   \n",
       "4   0.093503  ...          0.09        0.00        0.00        0.01   \n",
       "5   0.102469  ...          0.00        0.00        0.00        0.00   \n",
       "9   1.546257  ...          0.06        0.00        0.00        0.00   \n",
       "10  1.502713  ...          0.16        0.30        0.30        0.27   \n",
       "11  1.551380  ...          0.00        0.00        0.00        0.00   \n",
       "18  0.088380  ...          0.91        0.45        0.27        0.10   \n",
       "19  0.083256  ...          0.00        0.00        0.00        0.00   \n",
       "20  0.074290  ...          0.00        0.00        0.00        0.00   \n",
       "24  0.092217  ...          0.10        0.10        0.03        0.08   \n",
       "25  0.110148  ...          0.00        0.00        0.00        0.00   \n",
       "26  0.263843  ...          0.38        0.32        0.19        0.20   \n",
       "30  0.430535  ...          0.00        0.05        0.05        0.05   \n",
       "31  0.376751  ...          0.00        0.00        0.00        0.00   \n",
       "32  0.365226  ...          0.00        0.00        0.02        0.08   \n",
       "33  0.266405  ...          0.00        0.00        0.00        0.00   \n",
       "34  0.267685  ...          0.00        0.00        0.00        0.00   \n",
       "35  0.270247  ...          1.52        1.04        0.58        0.32   \n",
       "36  0.311251  ...          0.48        0.56        0.45        0.32   \n",
       "37  0.280510  ...          0.00        0.03        0.03        0.07   \n",
       "38  0.301004  ...          1.04        1.08        0.87        0.55   \n",
       "39  0.297410  ...          0.08        0.10        0.10        0.11   \n",
       "40  0.203913  ...          0.03        0.03        0.03        0.00   \n",
       "41  0.150119  ...          0.00        0.00        0.00        0.00   \n",
       "45  0.124237  ...          0.00        0.00        0.00        0.05   \n",
       "46  0.124237  ...          0.91        0.84        0.92        1.03   \n",
       "47  0.130641  ...          0.04        0.04        0.00        0.00   \n",
       "48  2.460329  ...          0.00        0.00        0.00        0.00   \n",
       "49  2.711379  ...          0.11        0.01        0.09        0.07   \n",
       "50  3.507408  ...          0.26        0.40        0.93        1.33   \n",
       "\n",
       "     AU45_r_95   AU45_r_96   AU45_r_97   AU45_r_98   AU45_r_99  label  \n",
       "0         0.51        0.27        0.04        0.00        0.00    0.0  \n",
       "1         0.09        0.29        0.31        0.42        0.27    1.0  \n",
       "2         0.00        0.00        0.00        0.00        0.00    2.0  \n",
       "3         0.02        0.13        0.18        0.30        0.20    0.0  \n",
       "4         0.01        0.02        0.09        0.10        0.10    1.0  \n",
       "5         0.00        0.00        0.00        0.00        0.00    2.0  \n",
       "9         0.00        0.00        0.00        0.00        0.02    0.0  \n",
       "10        0.16        0.07        0.02        0.02        0.02    1.0  \n",
       "11        0.00        0.00        0.00        0.00        0.00    2.0  \n",
       "18        0.10        0.03        0.00        0.00        0.06    0.0  \n",
       "19        0.00        0.00        0.00        0.00        0.00    1.0  \n",
       "20        0.00        0.00        0.00        0.00        0.00    2.0  \n",
       "24        0.08        0.18        0.10        0.18        0.18    0.0  \n",
       "25        0.00        0.00        0.00        0.00        0.00    1.0  \n",
       "26        0.24        0.30        0.30        0.17        0.05    2.0  \n",
       "30        0.00        0.08        0.08        0.08        0.02    0.0  \n",
       "31        0.38        0.53        0.70        0.45        0.42    1.0  \n",
       "32        0.10        0.09        0.04        0.02        0.01    2.0  \n",
       "33        0.00        0.05        0.10        0.10        0.05    0.0  \n",
       "34        0.00        0.70        1.15        1.94        1.82    1.0  \n",
       "35        0.13        0.00        0.00        0.00        0.00    2.0  \n",
       "36        0.24        0.23        0.38        0.36        0.30    0.0  \n",
       "37        0.05        0.05        0.00        0.00        0.00    1.0  \n",
       "38        0.28        0.28        0.47        0.68        0.59    2.0  \n",
       "39        0.12        0.07        0.03        0.00        0.03    0.0  \n",
       "40        0.00        0.00        0.00        0.00        0.01    1.0  \n",
       "41        0.00        0.00        0.00        0.00        0.00    2.0  \n",
       "45        0.05        0.11        0.14        0.17        0.25    0.0  \n",
       "46        1.03        1.12        1.13        1.15        1.04    1.0  \n",
       "47        0.00        0.00        0.00        0.00        0.00    2.0  \n",
       "48        0.00        0.00        0.00        0.00        0.00    0.0  \n",
       "49        0.07        0.00        0.02        0.02        0.02    1.0  \n",
       "50        1.29        0.76        0.36        0.00        0.00    2.0  \n",
       "\n",
       "[33 rows x 2594 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = pd.read_csv(\"empaticaDataFinal.csv\", sep=\",\")\n",
    "face = pd.read_csv(\"facial_expression.csv\", sep=\",\")\n",
    "T = pd.merge(emp, face, left_index=True, right_index=True)\n",
    "\n",
    "#random things\n",
    "#cols = list(T)\n",
    "#cols.insert(0, cols.pop(cols.index('participant'))) #bring participant to the front\n",
    "#T = T.loc[:, cols]\n",
    "T = T.drop(\"participant\", axis=1) #delete participant column, as it is not necessary..\n",
    "\n",
    "window = {'start': 0,'submit1': 1, 'submit2': 2}  #creating a dict file- label names to integers  \n",
    "#going through dataframe to label column and writing values where key matches \n",
    "T.label = [window[item] for item in T.label] \n",
    "T['label'] = T['label'].astype(np.float64)\n",
    "\n",
    "#delete rows that sklearn complains about\n",
    "T.replace([np.inf, -np.inf], np.nan) #things that are Inf or NaN, change to NaN\n",
    "T.dropna(inplace=True) #drop rows that contain NaN\n",
    "\n",
    "#get X = all data - label data, and y = label data\n",
    "X = T.drop('label', axis=1)\n",
    "y = T['label']\n",
    "\n",
    "#print(T.dtypes)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False):\n",
    "    C = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        per_class = np.diag(C) / C.sum(axis=1)\n",
    "    if np.any(np.isnan(per_class)):\n",
    "        warnings.warn('y_pred contains classes not in y_true')\n",
    "        per_class = per_class[~np.isnan(per_class)]\n",
    "    score = np.mean(per_class)\n",
    "    if adjusted:\n",
    "        n_classes = len(per_class)\n",
    "        chance = 1 / n_classes\n",
    "        score -= chance\n",
    "        score /= 1 - chance\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metr\n",
    "\n",
    "def model_evaluation(pipeline, train_data, train_labels, test_data, test_labels):\n",
    "    '''\n",
    "    This function returns training and testing scores obtained from a pipeline trained and tested on \n",
    "    training and testdef balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False):\n",
    "    C = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        per_class = np.diag(C) / C.sum(axis=1)\n",
    "    if np.any(np.isnan(per_class)):\n",
    "        warnings.warn('y_pred contains classes not in y_true')\n",
    "        per_class = per_class[~np.isnan(per_class)]\n",
    "    score = np.mean(per_class)\n",
    "    if adjusted:\n",
    "        n_classes = len(per_class)\n",
    "        chance = 1 / n_classes\n",
    "        score -= chance\n",
    "        score /= 1 - chance\n",
    "    return scoreing data+labels passed as input.\n",
    "    \n",
    "    =========================== ===============================================\n",
    "    Parameter                   Description\n",
    "    =========================== ===============================================\n",
    "    \"pipeline\"                  The pipeline to use to obtain performance\n",
    "                                metrics.\n",
    "    \"train_data\"                Training data (pandas dataframe). \n",
    "    \"train_labels\"              Training labels.\n",
    "    \"test_data\"                 Test data (pandas dataframe).\n",
    "    \"test_labels\"               Testing labels.                                \n",
    "    =========================== ===============================================\n",
    "    '''\n",
    "    \n",
    "    train_score = pipeline.score(train_data,train_labels)\n",
    "    train_predict = pipeline.predict(train_data)\n",
    "    train_F1 = metr.f1_score(y_pred=train_predict,y_true=train_labels,average='weighted')\n",
    "    train_bACC = balanced_accuracy_score(y_pred=train_predict,y_true=train_labels)\n",
    "    train_recall_weighted = metr.recall_score(y_pred=train_predict,y_true=train_labels,average='weighted')\n",
    "    \n",
    "    y_pred = pipeline.predict(test_data)\n",
    "    y_true = np.array(test_labels)\n",
    "    test_score = metr.accuracy_score(y_true, y_pred)\n",
    "    test_F1 = metr.f1_score(y_pred=y_pred,y_true=y_true,average='weighted')\n",
    "    test_bACC = balanced_accuracy_score(y_pred=y_pred,y_true=y_true)\n",
    "    \n",
    "    print(\"Training accuracy: \", train_score)    \n",
    "    print(\"Test accuracy: \", test_score)\n",
    "    print(\"Training F1-score: \", train_F1)\n",
    "    print(\"Test F1-score: \", test_F1)\n",
    "    print(\"Training balanced accuracy: \", train_bACC)    \n",
    "    print(\"Test balanced accuracy: \", test_bACC)\n",
    "\n",
    "    return train_score, test_score, y_pred, train_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "# Importing the dataset\n",
    "# Simple split\n",
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.30) \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM results: \n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier as lgb\n",
    "print(\"LightGBM results: \")\n",
    "\n",
    "lgbm = lgb(max_depth=10, num_iterations=1000, num_leaves=100, min_data_in_leaf=10, min_child_samples=5, \n",
    "           min_data=1, min_data_in_bin=1, num_class=3, learning_rate=0.0001,objective='multiclass', \n",
    "           boosting_type='gbdt', metric='multi_logloss', max_bin=1000, lambda_l2=3)\n",
    "p = Pipeline([\n",
    "                ('scaler', sc),\n",
    "                ('lgbm', lgbm)\n",
    "            ])\n",
    "p.fit(x_train,y_train)\n",
    "train_score, test_score, test_predict, train_predict = model_evaluation(p, x_train, y_train,\n",
    "                                                                  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost results: \n",
      "Training accuracy:  0.43478260869565216\n",
      "Test accuracy:  0.1\n",
      "Training F1-score:  0.2635046113306983\n",
      "Test F1-score:  0.01818181818181818\n",
      "Training balanced accuracy:  0.3333333333333333\n",
      "Test balanced accuracy:  0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(max_depth=100, n_estimators=100, objective='multi:softmax' , learning_rate=0.01, \n",
    "                    reg_alpha=1, max_leaves=20,min_child_weight=10,num_class=3)\n",
    "print(\"XGBoost results: \")\n",
    "p = Pipeline([\n",
    "                ('xgboost', xgb)\n",
    "            ])\n",
    "\n",
    "p.fit(x_train,y_train)\n",
    "train_score, test_score, test_predict, train_predict = model_evaluation(p, x_train, y_train,\n",
    "                                                                  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "def StratSplit(X,y):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    stratSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.25,random_state=42)\n",
    "    stratSplit.get_n_splits(X,y)\n",
    "    for train_idx,test_idx in stratSplit.split(X,y):\n",
    "        X_train=X[train_idx]\n",
    "        y_train=y[train_idx]\n",
    "        X_test=X[test_idx]\n",
    "        y_test=y[test_idx]    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"AllGlobalData.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_data.drop('label', axis=1)\n",
    "y = new_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = StratSplit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM results: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.8\n",
      "Test accuracy:  0.6428571428571429\n",
      "Training F1-score:  0.7978962131837306\n",
      "Test F1-score:  0.6428571428571427\n",
      "Training balanced accuracy:  0.7967032967032966\n",
      "Test balanced accuracy:  0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier as lgb\n",
    "print(\"LightGBM results: \")\n",
    "\n",
    "lgbm = lgb(max_depth=10, num_iterations=1000, num_leaves=100, min_data_in_leaf=10, num_class=3, learning_rate=0.0001,objective='multiclass', \n",
    "           boosting_type='gbdt', metric='multi_logloss', max_bin=100, lambda_l2=1)\n",
    "p = Pipeline([\n",
    "                ('lgbm', lgbm)\n",
    "            ])\n",
    "p.fit(x_train,y_train)\n",
    "train_score, test_score, test_predict, train_predict = model_evaluation(p, x_train, y_train,\n",
    "                                                                  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost results: \n",
      "Training accuracy:  0.325\n",
      "Test accuracy:  0.35714285714285715\n",
      "Training F1-score:  0.15943396226415096\n",
      "Test F1-score:  0.18796992481203006\n",
      "Training balanced accuracy:  0.3333333333333333\n",
      "Test balanced accuracy:  0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Softwares\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(max_depth=100, n_estimators=100, objective='multi:softmax' , learning_rate=0.01, \n",
    "                    reg_alpha=1, max_leaves=20,min_child_weight=10,num_class=3)\n",
    "print(\"XGBoost results: \")\n",
    "p = Pipeline([\n",
    "                ('xgboost', xgb)\n",
    "            ])\n",
    "\n",
    "p.fit(x_train,y_train)\n",
    "train_score, test_score, test_predict, train_predict = model_evaluation(p, x_train, y_train,\n",
    "                                                                  x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
